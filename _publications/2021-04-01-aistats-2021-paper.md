---
layout: single
title: "The Teaching Dimension of Kernel Perceptron"
authors: "A Kumar, H Zhang, A Singla, Y Chen"
venue: "The 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021)"
date: 2021-04-01
paperurl: https://proceedings.mlr.press/v130/kumar21a.html
pdfurl: https://proceedings.mlr.press/v130/kumar21a/kumar21a.pdf
arxiv: https://arxiv.org/abs/2010.14043
selected: true
abstract: |
	Algorithmic machine teaching has been studied primarily in linear settings where exact teaching is
	possible; comparatively little is known for nonlinear learners. We establish the sample complexity
	of teaching (teaching dimension) for kernelized perceptrons under different families of feature
	maps. As a warm-up, we show the teaching complexity is Θ(d) for exact teaching of linear
	perceptrons in R^d, and Θ(d^k) for kernel perceptrons with a degree-k polynomial kernel. Under
	smoothness assumptions on the data distribution, we provide a rigorous bound on the complexity of
	approximately teaching a Gaussian-kernel perceptron. We also give numerical examples of optimal
	(approximate) teaching sets in several canonical settings for linear, polynomial, and Gaussian
	kernels.
---

