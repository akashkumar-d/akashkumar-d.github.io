---
layout: single
title: "A Gap Between the Gaussian RKHS and Neural Networks: An Infinite-Center Asymptotic Analysis"
authors: "A Kumar, R Parhi, M Belkin"
venue: "The 38th Annual Conference on Learning Theory (COLT 2025)"
date: 2025-06-01
paperurl: https://proceedings.mlr.press/v291/kumar25b.html
arxiv: https://arxiv.org/abs/2502.16331
selected: true
abstract: |
  Recent works have characterized the function-space inductive bias of infinite-width bounded-norm
  single-hidden-layer neural networks as a kind of bounded-variation-type space. This novel neural
  network Banach space encompasses many classical multivariate function spaces, including certain
  Sobolev spaces and the spectral Barron spaces. Notably, this Banach space also includes functions
  that exhibit less classical regularity, such as those that only vary in a few directions. On bounded
  domains, it is well-established that the Gaussian reproducing kernel Hilbert space (RKHS) strictly
  embeds into this Banach space, demonstrating a clear gap between the Gaussian RKHS and the neural
  network Banach space. On unbounded domains (e.g., R^d), however, we show a fundamentally different
  picture: certain functions that lie in the Gaussian RKHS have infinite norm in the neural network
  Banach space. This yields a nontrivial gap between kernel methods and neural networks by exhibiting
  functions that kernel methods easily represent whereas neural networks cannot.
---

