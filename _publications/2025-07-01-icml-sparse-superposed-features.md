---
layout: single
title: "The Complexity of Learning Sparse Superposed Features with Feedback"
authors: "A Kumar"
venue: "The 42nd International Conference on Machine Learning (ICML 2025)"
date: 2025-07-01
paperurl: https://proceedings.mlr.press/v267/kumar25b.html
arxiv: https://arxiv.org/abs/2502.05407
selected: false
abstract: |
  The success of deep networks is crucially attributed to their ability to capture latent features
  within a representation space. We investigate whether the underlying learned features of a model
  can be efficiently retrieved through feedback from an agent (e.g., a large language model) in the
  form of relative triplet comparisons. These features may represent various constructs, including
  dictionaries in LLMs or components of a covariance matrix for Mahalanobis distances. We analyze the
  feedback complexity associated with learning a feature matrix in sparse settings. Our results
  establish tight bounds when the agent is permitted to construct activations and demonstrate strong
  upper bounds in sparse scenarios when the agentâ€™s feedback is limited to distributional information.
  We validate our theoretical findings through experiments on two applications: feature recovery from
  Recursive Feature Machine-trained models and dictionary extraction from sparse autoencoders trained
  on Large Language Models.
---

