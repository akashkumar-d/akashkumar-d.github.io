---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<div align="center">
  <p style="
      font-family: 'Cinzel', serif;
      font-size: 14px;
      font-weight: bold;
      border: 2px solid #FFD700; 
      padding: 10px; 
      display: inline-block;
      border-radius: 10px;
      background: linear-gradient(135deg, #1E1E1E, #3A3A3A);
      color: #FFD700;
      text-shadow: 1px 1px 3px #b8860b;
  ">
    तद्वनः मम हृदये वसति। <br>
    <i style="font-family: serif; font-weight: normal; text-transform: none;">Tadvanaḥ mama hṛdaye vasati.</i> <br>
  </p>
</div>

<link href="https://fonts.googleapis.com/css2?family=Cinzel:wght@400;700&display=swap" rel="stylesheet">


# About Me
I am a 5th-year doctoral candidate in the [Computer Science department](https://cse.ucsd.edu/) at the University of California San Diego where I am co-advised by Prof. [Sanjoy Dasgupta](https://cseweb.ucsd.edu/~dasgupta/) and Prof. [Misha Belkin](http://misha.belkin-wang.org/). I completed a BSc in Mathematics and Computer Science, followed by an MSc in Computer Science at Chennai Mathematical Institute (CMI, India). Previously, I have held positions as a research scientist intern/fellow at Adobe Research (San Jose, CA, USA), Max Planck Institute for Software Systems (Saarbrücken, Germany), and IBM Research (Bengaluru, IN). Throughout my education, I have been fortunate to receive several fellowships, including the **Jacobs School of Engineering Fellowship** (supporting my first year of doctoral studies at UCSD), the **Crerar Fellowship** (awarded as the strongest admit to the PhD program at UChicago CS, declined), the **Max Planck Institute Fellowship** (Fellow at MPI-SWS for 1.5 years), and the **Chennai Mathematical Institute Scholastic Fellowship** (five years).

I am broadly interested in advancing both the theoretical foundations and practical applications of machine learning. Specifically, my focus lies in statistical machine learning, algorithm design, interactive learning, optimization, and the theoretical aspects of deep learning. I am particularly enthusiastic about leveraging tools from probability theory, analysis, differential geometry, and statistics to rigorously study the computational and statistical efficiency of learning algorithms. My goal is to deepen our understanding of the principles underlying data-driven learning and the capabilities of machines to extract meaningful insights from complex datasets.<be>

More recently, I'm interested in the following problems (feel free to drop an email if you have interesting ideas to discuss):

- **Learning Distance Functions:**  
  Exploring the intersection of manifold learning, linear representation hypothesis, and classical distance learning.<br>
  [ArXiv 2025](https://arxiv.org/abs/2502.05407), [ArXiv 2024](https://arxiv.org/pdf/2412.01290)


- **Kernel Machines:**  
  Investigating the statistical-computational gap in learning with fixed or adaptive kernels.<br>
  [ArXiv 2024](https://arxiv.org/abs/2411.11242), [ArXiv 2021](https://arxiv.org/pdf/2010.14043.pdf)

- **Emergent Behavior in Neural Models:**  
  Studying phenomena such as grokking and phase transitions in neural architectures.<br>
  [Work in progress]()

- **Generalization with Non-Parametric Models:**  
  Addressing reliability and hallucination issues in models using simple yet interpretable classifiers like nearest neighbors.  
  <span style="color:blue"><i>Manuscript on selective classification with Neareat neighbors available</i></span>

  
**Contact**: (username id) akk002 at ucsd dot edu

# Recent News
<div class="recent-news-scroll" markdown="1">

1. [August, 2025] Presented my recent work at Princeton University and Yale University (Theory Seminar).
1. [May, 2025] Selected for the [Summer School](https://mlschool.princeton.edu/) on machine learning theory at Princeton University. 
1. [May, 2025] One paper to appear in JMLR 2025.
1. [May, 2025] Two papers accepted, one each in ICML 2025 and COLT 2025.
1. [Feb, 2025] Presenting my recent work at ITA'25 (San Diego) and reviewing for COLT'25.
1. [Dec, 2024] Attended [Unknown Futures of Generalization workshop](https://simons.berkeley.edu/workshops/unknown-futures-generalization) at Simons Institute, UC Berkeley.
1. [Summer, 2023] I was a research scientist intern at Adobe Research (San Jose, CA).
1. [Aug, 2022] Attended the [Deep learning theory workshop](https://simons.berkeley.edu/workshops/deep-learning-theory-workshop) at Simons Institute, UC Berkeley.
1. [July, 2022] Attended a summer school on Discrete Mathematics at Charles University, Prague (CZK).

</div>

# Preprints and Manuscripts

1. <b> Is Interpretability at Odds with Accuracy? Inapproximability of Decision Trees by Shallow Networks </b> <br>
**Akash Kumar** <br>
<i>In submission.</i><br>
[[Arxiv Coming Soon]()]

1. <b> Learning Smooth Distance Functions via Queries </b> <br>
**Akash Kumar**, Sanjoy Dasgupta <br>
<i>In submission to a conference.</i><br>
[[ArXiv 2024](https://arxiv.org/pdf/2412.01290)]

1. <b> Convergence of Nearest Neighbor Selective Classification </b> <br>
**Akash Kumar**, Sanjoy Dasgupta<br>
<i>Manuscript on request.</i>

1. <b> Average-case Complexity of Teaching Convex Polytopes via Halfspace Queries </b> <br>
**Akash Kumar**, [Adish Singla](https://machineteaching.mpi-sws.org/adishsingla.html), [Yisong Yue](http://www.yisongyue.com/), [Yuxin Chen](https://yuxinchen.org/).<br>
[[ArXiv 2020](https://arxiv.org/abs/2006.14677)]<br>
<i>Rejected from ICML 2021 with 6 [reviews](https://akashkumar-d.github.io/files/ICML'21.pdf)</i><br>
<i>Rejected from NeurlPS 2020 with 4 [reviews](https://akashkumar-d.github.io/files/NeurIPS'20.pdf)</i>

1. <b> Deletion to Induced Matching </b> <br>
**Akash Kumar**, Mithilesh Kumar.<br>
[[ArXiv 2020](https://arxiv.org/abs/2008.09660)]

# Publications

1. <b> A Gap Between the Gaussian RKHS and Neural Networks: An Infinite-Center Asymptotic Analysis </b> <br>
**Akash Kumar**, Rahul Parhi, Misha Belkin <br>
<i>The 38th Annual Conference on Learning Theory (COLT 2025).</i><br>
[[ArXiv 2025](https://arxiv.org/abs/2502.16331)]

1. <b> Mirror Descent on Reproducing Kernel Banach Space (RKBS) </b> <br>
**Akash Kumar**, Misha Belkin, Parthe Pandit <br>
<i>Journal of Machine Learning Research (JMLR), 2025. (To appear)</i><br>
[[ArXiv 2024](https://arxiv.org/abs/2411.11242)]

1. <b> The Complexity of Learning Sparse Superposed Features with Feedback </b> <br>
**Akash Kumar** <br>
<i>The 42nd International Conference on Machine Learning (ICML 2025).</i><br>
[[ArXiv 2025](https://arxiv.org/abs/2502.05407)]

1. <b> Robust Empirical Risk Minimization with Tolerance </b> <br>
Robi Bhattacharjee, Kamalika Chaudhuri, Max Hopkins, **Akash Kumar**, Hantao Yu (alphabetical order)<br>
<i>The 34th International Conference on Algorithmic Learning Theory (ALT'23), 2023.</i><br>
[[ArXiv 2023](https://arxiv.org/abs/2210.00635)]<br>
<span class="small">A preliminary version appeared in AdvML Frontiers @ ICML 2022.</span>

1. <b> Teaching via Best-Case Counterexamples in the Learning-with-Equivalence-Queries Paradigm </b> <br>
**Akash Kumar**, [Yuxin Chen](https://yuxinchen.org/), [Adish Singla](https://machineteaching.mpi-sws.org/adishsingla.html).<br>
<i>The 35th Conference on Neural Information Processing Systems (NeurIPS'21), 2021.</i><br>
[[Proc 2021](https://papers.nips.cc/paper/2021/file/e22dd5dabde45eda5a1a67772c8e25dd-Paper.pdf)] · [[OpenReview](https://openreview.net/forum?id=Ee7IOrpLwT)]

1. <b> The Teaching Dimension of Kernel Perceptrons </b> <br>
**Akash Kumar**, Hanqi Zhang, [Adish Singla](https://machineteaching.mpi-sws.org/adishsingla.html), [Yuxin Chen](https://yuxinchen.org/).<br>
<i>The 24th International Conference on Artificial Intelligence and Statistics (AISTATS'21), 2021.</i><br>
[[ArXiv 2021](https://arxiv.org/pdf/2010.14043.pdf)] · [[Proc 2021](http://proceedings.mlr.press/v130/kumar21a.html)]

# Recent talks
Learning Smooth Distance Functions via Queries (UCSD Presentation) [[Slides](https://drive.google.com/file/d/1vmprFyvcK6mb9zrEU9-55ZWij04WqkOz/view?usp=drive_link)]<br>
Feature Learning in Large Language Models (Adobe Research, San Jose)<br>
Teaching via Best-case Counterexamples (UCSD AI Seminar)

  
# Some notes
<b> [Improved Certified Adversarial Lower Bound Using Adaptive Relaxations](https://drive.google.com/file/d/1lZmiU3NnEhWHOtVuGhURxeFS4DWaYP_n/view?usp=sharing) </b> <br>
<i>Ongoing project on adversarial deep learning.</i>

<b> [Escaping Saddle Points and Tensor Decomposition](https://drive.google.com/file/d/1MAcwvvqGJCmr4VCnvE0kCFSTUB8w4mSA/view?usp=sharing) </b> <br>
<i>Master's Thesis under the guidance of Dr. K V Subrahmanyam. [[Slides](https://drive.google.com/file/d/1X4wGdlJvXqvzu-4C4qRFSEkSxy3ZF4Bg/view?usp=sharing)]</i>

<b> [Natural Proofs Vs Derandomization](https://drive.google.com/file/d/1TeHXyLIIUfp0p4iPqRqgNKwUx92ZO0Qn/view?usp=sharing) </b> <br>
<i>Project report completed as part of the Advanced Complexity course at Chennai Mathematical Institute.</i>
 
